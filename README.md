# Multimodal-Conversation

This repository is about papers related to multi-modal conversation. Since my research direction is currently focused on emotion-aware conversation, most papers are also related to this topic. a, v, t stands for audio, visual and textual modalities, respectively.

## Datasets
* M<sup>3</sup>ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database (ACL 2022 | citation: 7 | [paper](https://aclanthology.org/2022.acl-long.391.pdf) | [data](https://github.com/AIM3-RUC/RUCM3ED) | avt)
* IEMOCAP: Interactive emotional dyadic motion capture database (Lang Resour Eval 2008 | citation: 2798 | [paper](https://link.springer.com/article/10.1007/s10579-008-9076-6) | [data request form](https://github.com/Aditya3107/IEMOCAP_EMOTION_Recognition) | avt)
* MELD: Multimodal EmotionLines Dataset (2018 | citation: 678 | [paper](https://arxiv.org/pdf/1810.02508.pdf) | [data](https://affective-meld.github.io/) | avt)

## Modality Fusion
* Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition (EMNLP 2023 | [paper](https://arxiv.org/pdf/2310.06434v1.pdf) | [code](https://github.com/Srijith-rkr/Whispering-LLaMA) | at)

## Large Pretrained Multimodal Models
